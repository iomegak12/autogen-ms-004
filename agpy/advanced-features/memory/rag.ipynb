{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb776d91",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c331887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage, TextMessage\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\n",
    "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from indexers import SimpleDocumentIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff449c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "if not all([azure_openai_api_key, azure_openai_endpoint, azure_openai_deployment_name, azure_openai_api_version]):\n",
    "    raise ValueError(\"\"\"\n",
    "        Please set all required environment variables: \n",
    "            AZURE_OPENAI_API_KEY, \n",
    "            AZURE_OPENAI_ENDPOINT, \n",
    "            AZURE_OPENAI_DEPLOYMENT_NAME, \n",
    "            AZURE_OPENAI_API_VERSION\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3de0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment_name,\n",
    "    model=azure_openai_deployment_name,\n",
    "    api_key=azure_openai_api_key,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=azure_openai_api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f527ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_user_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"autogen_documentation\",\n",
    "        persistent_path=\"./autgen_chroma_db\",\n",
    "        k=3,\n",
    "        score_threshold=0.4,\n",
    "    )\n",
    ")\n",
    "\n",
    "await chroma_user_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa55812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def index_autogen_docs():\n",
    "    indexer = SimpleDocumentIndexer(\n",
    "        memory=chroma_user_memory,\n",
    "    )\n",
    "    \n",
    "    documents=[\n",
    "            \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html\",\n",
    "            \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html\",\n",
    "            \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md\"\n",
    "    ]\n",
    "    \n",
    "    chunks: int = await indexer.index_documents(documents)\n",
    "    \n",
    "    print(f\"Indexed {chunks} chunks from {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb06837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 69 chunks from 4 documents.\n"
     ]
    }
   ],
   "source": [
    "await index_autogen_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec21eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_assistant = AssistantAgent(\n",
    "    name=\"RAGAssistant\",\n",
    "    model_client=model_client,\n",
    "    memory=[chroma_user_memory],\n",
    "    system_message=\"You are a helpful assistant that can answer questions based on the provided context from the documentation.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abcd9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "What is AutoGen AgentChat?\n",
      "---------- MemoryQueryEvent (RAGAssistant) ----------\n",
      "[MemoryContent(content='arch Literature Review API Reference PyPi Source AgentChat Agents Agents # AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods: name : The unique name of the agent. description : The description of the agent in text. run : The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult . Agents are expected to be stateful and this method is expected to be called with new messages, not complete history . run_stream : Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item. See autogen_agentchat.messages for more information on AgentChat message types. Assistant Agent # AssistantAgent is a built-in agent that uses a language model and has the ability to use tools. Warning AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent . from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import StructuredMessage from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Define a tool that searches the web for information. # For simplicity,', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 1, 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'mime_type': 'MemoryMimeType.TEXT', 'score': 0.6033720374107361, 'id': '458d6e1a-2ae2-4a11-951c-57811493ebbd'}), MemoryContent(content='multi-agent applications.&#39;, name=&#39;web_search&#39;, call_id=&#39;call_703i17OLXfztkuioUbkESnea&#39;, is_error=False)], type=&#39;ToolCallExecutionEvent&#39;), ToolCallSummaryMessage(source=&#39;assistant&#39;, models_usage=None, metadata={}, content=&#39;AutoGen is a programming framework for building multi-agent applications.&#39;, type=&#39;ToolCallSummaryMessage&#39;)] The call to the run() method returns a TaskResult with the list of messages in the messages attribute, which stores the agent’s “thought process” as well as the final response. Note It is important to note that run() will update the internal state of the agent – it will add the messages to the agent’s message history. You can also call run() without a task to get the agent to generate responses given its current state. Note Unlike in v0.2 AgentChat, the tools are executed by the same agent directly within the same call to run() . By default, the agent will return the result of the tool call as the final response. Multi-Modal Input # The AssistantAgent can handle multi-modal input by providing the input as a MultiModalMessage . from io import BytesIO import PIL import requests from autogen_agentchat.messages import MultiModalMessage from autogen_core import Image # Create a multi-modal message with random image and text. pil_image = PIL . Image . open ( BytesIO ( requests . get ( &quot;https://picsum.photos/300/200&quot; ) . content )) img = Image ( pil_image ) multi_modal_message = MultiModalMessage', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 3, 'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.5938838422298431, 'id': '0112ee53-0f91-4c81-adce-bd4d7c4706b8'}), MemoryContent(content='anguage support for .NET and Python. - [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats. - [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution. The ecosystem also supports two essential _developer tools_: - [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications. - [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance. You can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling. With AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 3, 'source': 'https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md', 'score': 0.5473154783248901, 'id': 'c336f4d4-700c-4d3e-b2f1-5f1af68bcf94'})]\n",
      "---------- TextMessage (RAGAssistant) ----------\n",
      "AutoGen AgentChat is a programming framework designed for building multi-agent applications. It provides a set of preset agents, allowing developers to create agents that can respond to messages in various ways. Each agent has specific attributes and methods, such as:\n",
      "\n",
      "- **name**: A unique identifier for the agent.\n",
      "- **description**: A textual description of the agent's purpose.\n",
      "- **run**: A method to execute the agent with a given task or messages, returning a result.\n",
      "- **run_stream**: Similar to run but returns an iterator of messages, allowing for more dynamic responses.\n",
      "\n",
      "One of the built-in agents is the **AssistantAgent**, which utilizes a language model and can perform tasks like web searches. The framework supports both text and multi-modal input (e.g., combining images and text) through structured messaging. \n",
      "\n",
      "Additionally, AutoGen offers tools and extensions that enhance its capabilities, including a graphical user interface (AutoGen Studio) for building applications and benchmarking tools (AutoGen Bench) to evaluate agent performance. Overall, AutoGen AgentChat enables rapid prototyping and development of sophisticated multi-agent systems.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 13, 21, 59, 43, 293242, tzinfo=datetime.timezone.utc), content='What is AutoGen AgentChat?', type='TextMessage'), MemoryQueryEvent(source='RAGAssistant', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 13, 21, 59, 43, 736479, tzinfo=datetime.timezone.utc), content=[MemoryContent(content='arch Literature Review API Reference PyPi Source AgentChat Agents Agents # AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods: name : The unique name of the agent. description : The description of the agent in text. run : The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult . Agents are expected to be stateful and this method is expected to be called with new messages, not complete history . run_stream : Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item. See autogen_agentchat.messages for more information on AgentChat message types. Assistant Agent # AssistantAgent is a built-in agent that uses a language model and has the ability to use tools. Warning AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent . from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.messages import StructuredMessage from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient # Define a tool that searches the web for information. # For simplicity,', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 1, 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'mime_type': 'MemoryMimeType.TEXT', 'score': 0.6033720374107361, 'id': '458d6e1a-2ae2-4a11-951c-57811493ebbd'}), MemoryContent(content='multi-agent applications.&#39;, name=&#39;web_search&#39;, call_id=&#39;call_703i17OLXfztkuioUbkESnea&#39;, is_error=False)], type=&#39;ToolCallExecutionEvent&#39;), ToolCallSummaryMessage(source=&#39;assistant&#39;, models_usage=None, metadata={}, content=&#39;AutoGen is a programming framework for building multi-agent applications.&#39;, type=&#39;ToolCallSummaryMessage&#39;)] The call to the run() method returns a TaskResult with the list of messages in the messages attribute, which stores the agent’s “thought process” as well as the final response. Note It is important to note that run() will update the internal state of the agent – it will add the messages to the agent’s message history. You can also call run() without a task to get the agent to generate responses given its current state. Note Unlike in v0.2 AgentChat, the tools are executed by the same agent directly within the same call to run() . By default, the agent will return the result of the tool call as the final response. Multi-Modal Input # The AssistantAgent can handle multi-modal input by providing the input as a MultiModalMessage . from io import BytesIO import PIL import requests from autogen_agentchat.messages import MultiModalMessage from autogen_core import Image # Create a multi-modal message with random image and text. pil_image = PIL . Image . open ( BytesIO ( requests . get ( &quot;https://picsum.photos/300/200&quot; ) . content )) img = Image ( pil_image ) multi_modal_message = MultiModalMessage', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 3, 'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html', 'score': 0.5938838422298431, 'id': '0112ee53-0f91-4c81-adce-bd4d7c4706b8'}), MemoryContent(content='anguage support for .NET and Python. - [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats. - [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution. The ecosystem also supports two essential _developer tools_: - [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications. - [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance. You can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling. With AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'chunk_index': 3, 'source': 'https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md', 'score': 0.5473154783248901, 'id': 'c336f4d4-700c-4d3e-b2f1-5f1af68bcf94'})], type='MemoryQueryEvent'), TextMessage(source='RAGAssistant', models_usage=RequestUsage(prompt_tokens=1012, completion_tokens=224), metadata={}, created_at=datetime.datetime(2025, 6, 13, 21, 59, 48, 177064, tzinfo=datetime.timezone.utc), content=\"AutoGen AgentChat is a programming framework designed for building multi-agent applications. It provides a set of preset agents, allowing developers to create agents that can respond to messages in various ways. Each agent has specific attributes and methods, such as:\\n\\n- **name**: A unique identifier for the agent.\\n- **description**: A textual description of the agent's purpose.\\n- **run**: A method to execute the agent with a given task or messages, returning a result.\\n- **run_stream**: Similar to run but returns an iterator of messages, allowing for more dynamic responses.\\n\\nOne of the built-in agents is the **AssistantAgent**, which utilizes a language model and can perform tasks like web searches. The framework supports both text and multi-modal input (e.g., combining images and text) through structured messaging. \\n\\nAdditionally, AutoGen offers tools and extensions that enhance its capabilities, including a graphical user interface (AutoGen Studio) for building applications and benchmarking tools (AutoGen Bench) to evaluate agent performance. Overall, AutoGen AgentChat enables rapid prototyping and development of sophisticated multi-agent systems.\", type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = rag_assistant.run_stream(\n",
    "    task = \"What is AutoGen AgentChat?\",\n",
    ")\n",
    "\n",
    "await Console(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
